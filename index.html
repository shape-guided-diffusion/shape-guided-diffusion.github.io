<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shape-Guided Diffusion with Inside-Outside Attention</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <img width="64px" src="static/favicon.png">
            Shape-Guided Diffusion <br> with Inside-Outside Attention
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=_kJ-zUYAAAAJ&hl=en">Dong Huk Park</a><sup>1*</sup>,
          	</span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~graceluo/">Grace Luo</a><sup>1*</sup>,
          	</span>
          	<span class="author-block">
              Clayton Toste<sup>1</sup>,
          	</span>
          	<span class="author-block">
              <a href="https://scholar.google.com/citations?user=X0EXfT8AAAAJ&hl=en">Samaneh Azadi</a><sup>2</sup>,
          	</span>
          	<br>
          	<span class="author-block">
              <a href="https://xh-liu.github.io/">Xihui Liu</a><sup>1, 3</sup>,
          	</span>
          	<span class="author-block">
              Maka Karalashvili<sup>4</sup>,
          	</span>
          	<span class="author-block">
              <a href="https://anna-rohrbach.net/">Anna Rohrbach</a><sup>1</sup>,
          	</span>
          	<span class="author-block">
              <a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>1</sup>,
          	</span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Berkeley,</span>
            <br>
            <span class="author-block"><sup>2</sup>Meta AI,</span>
            <span class="author-block"><sup>3</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>4</sup>BMW Group</span>
            <br>
            <span class="is-size-6">*Denotes equal contribution</span>
            <br><br>
            <span class="author-block">WACV 2024</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2212.00210"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/shape-guided-diffusion/shape-guided-diffusion"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/shape-guided-diffusion/shape-guided-diffusion/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-rocket"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
	  	  <img src="static/figures/teaser.png">
	    </figure>
      <span class="is-size-6 has-text-centered">
        We demonstrate the importance of using an explicit shape when performing a local edit on a real image. Prior work has difficulty preserving the source object's shape, even when adapted for local editing. We propose Shape-Guided Diffusion, a training-free method that uses a novel Inside-Outside Attention mechanism to delineate which spatial regions are object vs. background and ensure that edits are localized to the correct region.
        Our method can be provided an object mask as input or infer a mask from text, as is shown in the above example.
      </span>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="has-text-centered">Examples of shape-guided edits synthesized by our method using automatically inferred masks.</div>
      <div id="results-carousel-face" class="carousel results-carousel" data-slides-to-show="1">
        <div class="item-1">
        <img src="static/figures/carousel/1.png">
        </div>
        <div class="item-2">
        <img src="static/figures/carousel/2.png">
        </div>
        <div class="item-3">
      	<img src="static/figures/carousel/3.png">
        </div>
        <div class="item-4">
        <img src="static/figures/carousel/4.png">
        </div>
        <div class="item-5">
        <img src="static/figures/carousel/5.png">
        </div>
        <div class="item-6">
        <img src="static/figures/carousel/6.png">
        </div>
        <div class="item-7">
        <img src="static/figures/carousel/7.png">
        </div>
        <div class="item-8">
        <img src="static/figures/carousel/8.png">
        </div>
        <div class="item-9">
        <img src="static/figures/carousel/9.png">
        </div>
        <div class="item-10">
        <img src="static/figures/carousel/10.png">
        </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region.
            We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degradation in text alignment or image realism according to both automatic metrics and annotator ratings.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- A -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Shape-Guided Diffusion</h2>
        <div class="content has-text-justified">
          <p>
            Our method takes a real image, source prompt ("dog"), edit prompt ("dog wearing a colorful shirt"), as well as an optional object mask (inferred from the source prompt if not provided), and outputs an edited image. Left: we modify a frozen pretrained text-to-image diffusion model during both the inversion and generation processes. Right: we show a detailed view of one layer in the U-Net, where Inside-Outside Attention constrains the self- and cross-attention maps according to the mask.
          </p>
        </div>
        <figure class="image">
	  	  <img src="static/figures/overall-pipeline.png">
	  	</figure>
      </div>
    </div>
    <!--/ A -->
    <br>
    <!-- B -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Inside-Outside Attention</h2>
        <div class="content has-text-justified">
          <p>
            In our Inside-Outside Attention mechanism, we modify the attention maps in both the cross-attention and self-attention layers. In the cross-attention layer depending on whether the text embedding refers to the inside or outside the object, we constrain the attention map M according to the object mask or the inverted object mask to produce M'. In the self-attention layer we perform a similar operation on the inside and outside pixel embeddings.
          </p>
          <figure class="image">
        	<img style="width:50vw !important;" src="static/figures/inside-outside-attention.png">
          </figure>
        </div>
      </div>
    </div>
    <!--/ B -->
    <br>
    <!-- C -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Additional Editing Capabilities</h2>
        
        <div class="content has-text-justified">
          <p>
            <b>Inter-Class Edits.</b> Our method can perform both intra- and inter-class edits to the same object.
          </p>
          <figure class="image">
	  		<img style="width:50vw !important;" src="static/figures/inter-class-edit.png">
	  	  </figure>
        </div>
        
        <div class="content has-text-justified">
          <p>
            <b>Background Edits.</b> Our method is not only able to edit objects but also backgrounds.
          </p>
          <figure class="image">
	  	  	<img src="static/figures/background-edit.png">
	  	  </figure>
        </div>

        <div class="content has-text-justified">
          <p>
            <b>Simultaneous Inside-Outside Edits.</b> Since our method allows us to localize and separate the editing of the object and background, we can simultaneously edit both with two different prompts. This property allows us to maintain the relation of the object with the scene.
          </p>
          <figure class="image">
	  	  	<img src="static/figures/simultaneous-edit.png">
	  	  </figure>
        </div>
        
      </div>
    </div>
    <!--/ C -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @inproceedings{park2024shape,
      author    = {Dong Huk Park* and Grace Luo* and Clayton Toste and Samaneh Azadi and Xihui Liu and Maka Karalashvili and Anna Rohrbach and Trevor Darrell},
      title     = {Shape-Guided Diffusion with Inside-Outside Attention},
      booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
      year      = {2024},
    }
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>. We build on top of the source code from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
